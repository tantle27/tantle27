{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPbhR6W9o5DxL+1larlmmZX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tantle27/tantle27/blob/main/Final_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Imports and Setup\n"
      ],
      "metadata": {
        "id": "Z7PRNvNMRn38"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "LCECJxmargYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132a3537-574e-4ec4-f3ab-693a428cdfef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (3.13.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from h5py) (2.0.2)\n",
            "Requirement already satisfied: pca in /usr/local/lib/python3.11/dist-packages (2.0.9)\n",
            "Requirement already satisfied: datazets in /usr/local/lib/python3.11/dist-packages (from pca) (1.1.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from pca) (0.14.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pca) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pca) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pca) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pca) (1.14.1)\n",
            "Requirement already satisfied: colourmap>=1.1.19 in /usr/local/lib/python3.11/dist-packages (from pca) (1.1.20)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from pca) (2.2.2)\n",
            "Requirement already satisfied: scatterd>=1.3.7 in /usr/local/lib/python3.11/dist-packages (from pca) (1.3.7)\n",
            "Requirement already satisfied: adjusttext in /usr/local/lib/python3.11/dist-packages (from pca) (1.3.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from scatterd>=1.3.7->pca) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from datazets->pca) (2.32.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pca) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pca) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pca) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pca) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pca) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pca) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pca) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pca) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->pca) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->pca) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pca) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pca) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->pca) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->pca) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->datazets->pca) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->datazets->pca) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->datazets->pca) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->datazets->pca) (2025.1.31)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "--2025-04-13 20:33:12--  http://ann-benchmarks.com/sift-128-euclidean.hdf5\n",
            "Resolving ann-benchmarks.com (ann-benchmarks.com)... 104.21.96.1, 104.21.112.1, 104.21.32.1, ...\n",
            "Connecting to ann-benchmarks.com (ann-benchmarks.com)|104.21.96.1|:80... connected.\n",
            "HTTP request sent, awaiting response... 416 Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Efficient ANN Search with SOAR (Orthogonal Residuals) and FAISS\n",
        "\n",
        "This script implements the following key components:\n",
        "1. Data Loading and Normalization (from an HDF5 dataset)\n",
        "2. PCA Transformation on training and testing data\n",
        "3. Partitioning (Clustering) of data using MiniBatchKMeans\n",
        "4. Definition of the SOAR Model with multiple residual encoders\n",
        "5. Training of the SOAR Model with a combined MSE and Orthogonality Loss\n",
        "6. Building a FAISS HNSW index on the encoded representations\n",
        "7. Dynamic Reranking and Recall Evaluation for ANN retrieval\n",
        "8. Hyperparameter Tuning Functions for clustering and lambda (orthogonality strength)\n",
        "\n",
        "Required installations:\n",
        "    !pip install faiss-cpu\n",
        "    !pip install scikit-learn\n",
        "    !pip install h5py\n",
        "    !pip install pca  # if needed; check if PCA from scikit-learn suffices\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "!pip install scikit-learn\n",
        "!pip install h5py\n",
        "!pip install pca\n",
        "!pip install faiss-cpu\n",
        "!wget --continue http://ann-benchmarks.com/sift-128-euclidean.hdf5\n",
        "\n",
        "import time\n",
        "import psutil\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import faiss\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import h5py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Data Loading and Preprocessing\n"
      ],
      "metadata": {
        "id": "oVE9-TNbQqRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sift_hdf5(file_path, dataset_name='data'):\n",
        "    \"\"\"Load data from an HDF5 file.\"\"\"\n",
        "    with h5py.File(file_path, 'r') as f:\n",
        "        data = np.array(f[dataset_name])\n",
        "    return data\n",
        "\n",
        "def load_and_normalize_data_hdf5(file_path, dataset_name='data'):\n",
        "    \"\"\"Load data and normalize it to unit length.\"\"\"\n",
        "    data = load_sift_hdf5(file_path, dataset_name)\n",
        "    norms = np.linalg.norm(data, axis=1, keepdims=True)\n",
        "    return data / (norms + 1e-8)\n",
        "\n",
        "def apply_pca(train_data, test_data, n_components=96):\n",
        "    \"\"\"Apply PCA transformation to reduce dimensionality.\"\"\"\n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca.fit(train_data)\n",
        "    train_data_pca = pca.transform(train_data)\n",
        "    test_data_pca = pca.transform(test_data)\n",
        "    return train_data_pca.astype('float32'), test_data_pca.astype('float32')"
      ],
      "metadata": {
        "id": "vwPK8dCkQoSB"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Section 3: Data Partitioning and Residual Computation\n"
      ],
      "metadata": {
        "id": "maRL5N6tQu5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partition_data(data, n_clusters=256, random_state=42):\n",
        "    \"\"\"\n",
        "    Partition data using MiniBatchKMeans clustering.\n",
        "    Returns cluster labels and cluster centers.\n",
        "    \"\"\"\n",
        "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=random_state, batch_size=1024)\n",
        "    labels = kmeans.fit_predict(data)\n",
        "    centers = kmeans.cluster_centers_.astype('float32')\n",
        "    return labels, centers\n"
      ],
      "metadata": {
        "id": "qoO3f-jUQumW"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Section 4: SOAR Model Definition (Residual Encoders)\n"
      ],
      "metadata": {
        "id": "E57x0sTyQ02I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ResidualEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple fully connected layer to transform residual vectors.\n",
        "    The output is L2 normalized.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, rep_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, rep_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        rep = self.fc(x)\n",
        "        rep = rep / (rep.norm(dim=1, keepdim=True) + 1e-8)\n",
        "        return rep\n",
        "\n",
        "class SOARModel(nn.Module):\n",
        "    \"\"\"\n",
        "    The SOAR model uses multiple ResidualEncoder blocks to produce\n",
        "    orthogonal components. The outputs are aggregated to form the final representation.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, rep_dim, num_reps=4):\n",
        "        super().__init__()\n",
        "        self.encoders = nn.ModuleList([ResidualEncoder(input_dim, rep_dim) for _ in range(num_reps)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        reps = [encoder(x) for encoder in self.encoders]\n",
        "        # Sum all the representations to produce the combined representation\n",
        "        combined_rep = torch.stack(reps, dim=0).sum(dim=0)\n",
        "        return combined_rep, reps\n",
        "\n",
        "def orthogonality_loss(reps):\n",
        "    \"\"\"\n",
        "    Compute the orthogonality loss as the mean absolute dot product\n",
        "    between each distinct pair of encoder outputs.\n",
        "    \"\"\"\n",
        "    loss = 0.0\n",
        "    num_reps = len(reps)\n",
        "    for i in range(num_reps):\n",
        "        for j in range(i + 1, num_reps):\n",
        "            dot_product = (reps[i] * reps[j]).sum(dim=1)\n",
        "            loss += torch.mean(torch.abs(dot_product))\n",
        "    return loss\n",
        "\n",
        "def soar_loss(target, pred, reps, lam=0.1):\n",
        "    \"\"\"\n",
        "    Combined loss: Mean Squared Error between the aggregated prediction\n",
        "    and the target, plus a weighted orthogonality loss.\n",
        "    \"\"\"\n",
        "    mse = nn.MSELoss()(pred, target)\n",
        "    ortho = orthogonality_loss(reps)\n",
        "    return mse + lam * ortho"
      ],
      "metadata": {
        "id": "5hbuWk4nQ4Tj"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Section 5: Training Function for the SOAR Model\n"
      ],
      "metadata": {
        "id": "4LJvccbtQ51h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_soar_model(data, labels, centers, input_dim, rep_dim, num_reps=4, epochs=5, batch_size=1024, lr=1e-4, lam=0.1):\n",
        "    \"\"\"\n",
        "    Train the SOAR model using residual vectors computed as data minus cluster centers.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = SOARModel(input_dim, rep_dim, num_reps=num_reps).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "    # Convert data to tensors and compute residuals\n",
        "    data_t = torch.tensor(data, dtype=torch.float32, device=device)\n",
        "    centers_t = torch.tensor(centers, dtype=torch.float32, device=device)\n",
        "    labels_t = torch.tensor(labels, dtype=torch.long, device=device)\n",
        "    residuals = data_t - centers_t[labels_t]\n",
        "\n",
        "    n_samples = residuals.shape[0]\n",
        "    for epoch in range(epochs):\n",
        "        perm = torch.randperm(n_samples, device=device)\n",
        "        epoch_loss = 0.0\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            idx = perm[i: i + batch_size]\n",
        "            batch = residuals[idx]\n",
        "            optimizer.zero_grad()\n",
        "            combined_rep, reps = model(batch)\n",
        "            loss_val = soar_loss(batch, combined_rep, reps, lam=lam)\n",
        "            loss_val.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss_val.item() * batch.size(0)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss/n_samples:.4f}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "32GRAMGOQ8Sb"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Section 6: FAISS Index Building and Querying Functions\n"
      ],
      "metadata": {
        "id": "Qx-yZC3ZQ-ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_faiss_index(reps, batch_size=10000, efSearch_val =50):\n",
        "    \"\"\"\n",
        "    Build a FAISS HNSW index on the provided representations with a specified efSearch value.\n",
        "    \"\"\"\n",
        "    rep_dim = reps.shape[1]\n",
        "    index = faiss.IndexHNSWFlat(rep_dim, 128)\n",
        "    index.hnsw.efConstruction = 50\n",
        "    index.hnsw.efSearch = efSearch_val\n",
        "\n",
        "    num_batches = (reps.shape[0] + batch_size - 1) // batch_size\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i + 1) * batch_size, reps.shape[0])\n",
        "        index.add(reps[start:end])\n",
        "    return index\n",
        "\n",
        "def dynamic_rerank_count(query_vector, faiss_index, max_candidates=500, base_min=50):\n",
        "    \"\"\"\n",
        "    Determine the candidate pool size for dynamic re-ranking based on query distance distribution.\n",
        "    \"\"\"\n",
        "    distances, candidate_indices = faiss_index.search(query_vector, max_candidates)\n",
        "    distances = distances[0]\n",
        "    k = 10\n",
        "    if k < len(distances) - 1:\n",
        "        gap = distances[k] - distances[k - 1]\n",
        "    else:\n",
        "        gap = 0\n",
        "\n",
        "    std_topk = np.std(distances[:k])\n",
        "    dynamic_count = base_min\n",
        "    if std_topk < 0.01:\n",
        "        dynamic_count = min(max_candidates, base_min + int((0.01 - std_topk) * 1000))\n",
        "    dynamic_count = max(dynamic_count, k)\n",
        "    return dynamic_count, candidate_indices[0][:dynamic_count]\n",
        "\n",
        "def evaluate_recall(faiss_index, base_data, test_data, true_neighbors, k=10, max_rerank=500):\n",
        "    \"\"\"\n",
        "    Evaluate the recall@k for the FAISS index using dynamic re-ranking.\n",
        "    \"\"\"\n",
        "    n_queries = test_data.shape[0]\n",
        "    total_recall = 0.0\n",
        "    for i in range(n_queries):\n",
        "        query_vec = test_data[i:i+1]\n",
        "        dynamic_count, dynamic_candidates = dynamic_rerank_count(query_vec, faiss_index, max_candidates=max_rerank)\n",
        "        distances = np.linalg.norm(query_vec - base_data[dynamic_candidates], axis=1)\n",
        "        retrieved_indices = dynamic_candidates[np.argsort(distances)][:k]\n",
        "        gt = set(true_neighbors[i, :k])\n",
        "        overlap = len(set(retrieved_indices) & gt)\n",
        "        total_recall += overlap / k\n",
        "    recall = total_recall / n_queries\n",
        "    print(f\"Avg Recall@{k}: {recall:.4f}\")\n",
        "    return recall\n"
      ],
      "metadata": {
        "id": "CmE92IAIRBMF"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Section 7: Hyperparameter Tuning Functions\n"
      ],
      "metadata": {
        "id": "P_TVnpXmRDSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hyperparam_search_clustering(train_data, test_data, true_neighbors, cluster_options=[128, 256, 512], device=None):\n",
        "    best_recall = 0\n",
        "    best_cluster_count = None\n",
        "    for n_clusters in cluster_options:\n",
        "        labels, centers = partition_data(train_data, n_clusters=n_clusters)\n",
        "        model = train_soar_model(train_data, labels, centers, input_dim=train_data.shape[1], rep_dim=train_data.shape[1], num_reps = 6, epochs=5, batch_size=1024)\n",
        "        train_data_tensor = torch.tensor(train_data, dtype=torch.float32, device=device)\n",
        "        reps_train, _ = model(train_data_tensor)\n",
        "        reps_train = reps_train.cpu().detach().numpy()\n",
        "        index = build_faiss_index(reps_train)\n",
        "        current_recall = evaluate_recall(index, train_data, test_data, true_neighbors, k=10)\n",
        "        print(f\"Clusters={n_clusters}, Recall@10={current_recall:.4f}\")\n",
        "        if current_recall > best_recall:\n",
        "            best_recall = current_recall\n",
        "            best_cluster_count = n_clusters\n",
        "    print(f\"Best cluster count: {best_cluster_count} with recall {best_recall:.4f}\")\n",
        "    return best_cluster_count\n",
        "\n",
        "def tune_orthogonality(train_data, labels, centers, test_data, true_neighbors, lambdas=[0.01, 0.05, 0.1, 0.001], input_dim=64, rep_dim=64, num_reps=6, epochs=5, batch_size=1024, lr=1e-4):\n",
        "    best_lambda = None\n",
        "    best_recall = 0\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    for lam in lambdas:\n",
        "        model = train_soar_model(train_data, labels, centers, input_dim, rep_dim, num_reps, epochs, batch_size, lr, lam=lam)\n",
        "        train_data_tensor = torch.tensor(train_data, dtype=torch.float32, device=device)\n",
        "        reps_train, _ = model(train_data_tensor)\n",
        "        reps_train = reps_train.cpu().detach().numpy()\n",
        "        index = build_faiss_index(reps_train)\n",
        "        current_recall = evaluate_recall(index, train_data, test_data, true_neighbors, k=10)\n",
        "        print(f\"Lambda={lam}, Recall@10={current_recall:.4f}\")\n",
        "        if current_recall > best_recall:\n",
        "            best_recall = current_recall\n",
        "            best_lambda = lam\n",
        "    print(f\"Best lambda: {best_lambda} with recall {best_recall:.4f}\")\n",
        "    return best_lambda, best_recall\n"
      ],
      "metadata": {
        "id": "KDfyuCCQRHDd"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Section 8: Main Pipeline\n"
      ],
      "metadata": {
        "id": "ozLGVwDHRNw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    h5_path = \"sift-128-euclidean.hdf5\"\n",
        "\n",
        "    print(\"Loading and normalizing data...\")\n",
        "    train_data = load_and_normalize_data_hdf5(h5_path, 'train')\n",
        "    test_data = load_and_normalize_data_hdf5(h5_path, 'test')\n",
        "    with h5py.File(h5_path, 'r') as f:\n",
        "        true_neighbors = np.array(f['neighbors'])\n",
        "\n",
        "    print(\"Applying PCA transformation...\")\n",
        "    train_data, test_data = apply_pca(train_data, test_data, n_components=128)\n",
        "\n",
        "    print(\"Partitioning training data via clustering...\")\n",
        "    best_cluster_count = 128  #got from parameter testing\n",
        "    best_lambda = 0.001\n",
        "\n",
        "    labels, centers = partition_data(train_data, n_clusters=best_cluster_count)\n",
        "    print(\"Training the SOAR model...\")\n",
        "    model = train_soar_model(\n",
        "        train_data, labels, centers,\n",
        "        input_dim=train_data.shape[1], rep_dim=128, lam=best_lambda, epochs=5\n",
        "    )\n",
        "\n",
        "    print(\"Generating training representations using the SOAR model...\")\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    train_data_tensor = torch.tensor(train_data, dtype=torch.float32, device=device)\n",
        "    reps_train, _ = model(train_data_tensor)\n",
        "    reps_train = reps_train.cpu().detach().numpy()\n",
        "\n",
        "    print(\"Generating test representations using the SOAR model...\")\n",
        "    test_data_tensor = torch.tensor(test_data, dtype=torch.float32, device=device)\n",
        "    reps_test, _ = model(test_data_tensor)\n",
        "    reps_test = reps_test.cpu().detach().numpy()\n",
        "\n",
        "    # Testing the effect of different efSearch values\n",
        "    efSearch_values = [50, 100, 200]\n",
        "    results = {}\n",
        "    for ef in efSearch_values:\n",
        "        print(f\"\\nBuilding FAISS index with efSearch = {ef}...\")\n",
        "        index = build_faiss_index(reps_train, efSearch_val=ef)\n",
        "        print(\"Evaluating Recall@10 on test data...\")\n",
        "        start_time = time.time()\n",
        "        recall = evaluate_recall(index, reps_train, reps_test, true_neighbors, k=10)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"efSearch = {ef} -- Recall@10: {recall:.4f}, Evaluation time: {elapsed_time:.4f} seconds\")\n",
        "        results[ef] = {'Recall@10': recall, 'EvalTime(s)': elapsed_time}\n",
        "\n",
        "    print(\"\\nSummary of efSearch evaluations:\")\n",
        "    for ef, metrics in results.items():\n",
        "        print(f\"efSearch = {ef}: Recall@10 = {metrics['Recall@10']:.4f}, Evaluation Time = {metrics['EvalTime(s)']:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxGCggkuPiPJ",
        "outputId": "b9851a8c-fb15-4aa9-e848-75fb75967100"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and normalizing data...\n",
            "Applying PCA transformation...\n",
            "Partitioning training data via clustering...\n",
            "Training the SOAR model...\n",
            "Epoch 1/5 - Loss: 0.0082\n",
            "Epoch 2/5 - Loss: 0.0022\n",
            "Epoch 3/5 - Loss: 0.0019\n",
            "Epoch 4/5 - Loss: 0.0019\n",
            "Epoch 5/5 - Loss: 0.0019\n",
            "Generating training representations using the SOAR model...\n",
            "Generating test representations using the SOAR model...\n",
            "\n",
            "Building FAISS index with efSearch = 50...\n",
            "Evaluating Recall@10 on test data...\n",
            "Avg Recall@10: 0.9467\n",
            "efSearch = 50 -- Recall@10: 0.9467, Evaluation time: 5.5634 seconds\n",
            "\n",
            "Building FAISS index with efSearch = 100...\n",
            "Evaluating Recall@10 on test data...\n",
            "Avg Recall@10: 0.9522\n",
            "efSearch = 100 -- Recall@10: 0.9522, Evaluation time: 7.6918 seconds\n",
            "\n",
            "Building FAISS index with efSearch = 200...\n",
            "Evaluating Recall@10 on test data...\n",
            "Avg Recall@10: 0.9536\n",
            "efSearch = 200 -- Recall@10: 0.9536, Evaluation time: 11.1224 seconds\n",
            "\n",
            "Summary of efSearch evaluations:\n",
            "efSearch = 50: Recall@10 = 0.9467, Evaluation Time = 5.5634 seconds\n",
            "efSearch = 100: Recall@10 = 0.9522, Evaluation Time = 7.6918 seconds\n",
            "efSearch = 200: Recall@10 = 0.9536, Evaluation Time = 11.1224 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 9 Parameter Testing"
      ],
      "metadata": {
        "id": "ntlNFMxJvXIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running hyperparameter search for clustering...\")\n",
        "best_cluster_count = hyperparam_search_clustering(train_data, test_data, true_neighbors,\n",
        "                                                  cluster_options=[128, 256, 512], device=device)\n",
        "print(f\"Optimal cluster count: {best_cluster_count}\")\n",
        "\n",
        "#Hyperparameter tuning for orthogonality strength (lambda)\n",
        "print(\"Tuning orthogonality strength (lambda)...\")\n",
        "best_lambda, best_recall = tune_orthogonality(train_data, labels, centers,\n",
        "                                              test_data, true_neighbors,\n",
        "                                              lambdas=[0.01, 0.05, 0.001],\n",
        "                                              input_dim=train_data.shape[1],\n",
        "                                              rep_dim=train_data.shape[1],\n",
        "                                              num_reps=6, epochs=5,\n",
        "                                              batch_size=1024, lr=1e-4)\n",
        "print(f\"Optimal lambda: {best_lambda} with Recall@10: {best_recall:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUbccS5WusyU",
        "outputId": "a0ac98d0-5a83-4378-bff9-270b777c3b8e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running hyperparameter search for clustering...\n",
            "Epoch 1/5 - Loss: 0.0626\n",
            "Epoch 2/5 - Loss: 0.0493\n",
            "Epoch 3/5 - Loss: 0.0469\n",
            "Epoch 4/5 - Loss: 0.0462\n",
            "Epoch 5/5 - Loss: 0.0458\n",
            "Avg Recall@10: 0.6202\n",
            "Clusters=128, Recall@10=0.6202\n",
            "Epoch 1/5 - Loss: 0.0614\n",
            "Epoch 2/5 - Loss: 0.0492\n",
            "Epoch 3/5 - Loss: 0.0470\n",
            "Epoch 4/5 - Loss: 0.0463\n",
            "Epoch 5/5 - Loss: 0.0461\n",
            "Avg Recall@10: 0.6274\n",
            "Clusters=256, Recall@10=0.6274\n",
            "Epoch 1/5 - Loss: 0.0610\n",
            "Epoch 2/5 - Loss: 0.0492\n",
            "Epoch 3/5 - Loss: 0.0471\n",
            "Epoch 4/5 - Loss: 0.0465\n",
            "Epoch 5/5 - Loss: 0.0463\n",
            "Avg Recall@10: 0.6207\n",
            "Clusters=512, Recall@10=0.6207\n",
            "Best cluster count: 256 with recall 0.6274\n",
            "Optimal cluster count: 256\n",
            "Tuning orthogonality strength (lambda)...\n",
            "Epoch 1/5 - Loss: 0.0329\n",
            "Epoch 2/5 - Loss: 0.0276\n",
            "Epoch 3/5 - Loss: 0.0268\n",
            "Epoch 4/5 - Loss: 0.0267\n",
            "Epoch 5/5 - Loss: 0.0267\n",
            "Avg Recall@10: 0.3949\n",
            "Lambda=0.01, Recall@10=0.3949\n",
            "Epoch 1/5 - Loss: 0.0542\n",
            "Epoch 2/5 - Loss: 0.0465\n",
            "Epoch 3/5 - Loss: 0.0449\n",
            "Epoch 4/5 - Loss: 0.0440\n",
            "Epoch 5/5 - Loss: 0.0425\n",
            "Avg Recall@10: 0.8245\n",
            "Lambda=0.05, Recall@10=0.8245\n",
            "Epoch 1/5 - Loss: 0.0092\n",
            "Epoch 2/5 - Loss: 0.0030\n",
            "Epoch 3/5 - Loss: 0.0029\n",
            "Epoch 4/5 - Loss: 0.0029\n",
            "Epoch 5/5 - Loss: 0.0029\n",
            "Avg Recall@10: 0.9823\n",
            "Lambda=0.001, Recall@10=0.9823\n",
            "Best lambda: 0.001 with recall 0.9823\n",
            "Optimal lambda: 0.001 with Recall@10: 0.9823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca_dims = [64, 96, 128, None]\n",
        "results_pca = []\n",
        "\n",
        "for dim in pca_dims:\n",
        "    print(f\"\\n--- PCA = {dim if dim else 'None'} ---\")\n",
        "    if dim is None:\n",
        "        train_data_pca, test_data_pca = train_data, test_data\n",
        "    else:\n",
        "        train_data_pca, test_data_pca = apply_pca(train_data, test_data, n_components=dim)\n",
        "\n",
        "    labels, centers = partition_data(train_data_pca, n_clusters=256)\n",
        "    model = train_soar_model(train_data_pca, labels, centers,\n",
        "                             input_dim=train_data_pca.shape[1],\n",
        "                             rep_dim=train_data_pca.shape[1],\n",
        "                             lam=0.05)\n",
        "\n",
        "    reps_train, _ = model(torch.tensor(train_data_pca, dtype=torch.float32, device=device))\n",
        "    reps_test, _ = model(torch.tensor(test_data_pca, dtype=torch.float32, device=device))\n",
        "\n",
        "    reps_train = reps_train.cpu().detach().numpy()\n",
        "    reps_test = reps_test.cpu().detach().numpy()\n",
        "\n",
        "    index = build_faiss_index(reps_train)\n",
        "    start = time.time()\n",
        "    recall = evaluate_recall(index, reps_train, reps_test, true_neighbors)\n",
        "    query_time = time.time() - start\n",
        "\n",
        "    results_pca.append((dim if dim else 'None', recall, query_time))\n",
        "    print(f\"PCA {dim}D → Recall@10: {recall:.4f}, Query Time: {query_time:.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKvzi46_NxKM",
        "outputId": "a40d82d0-4d33-4aed-ed97-d40cd34f31eb"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- PCA = 64 ---\n",
            "Epoch 1/5 - Loss: 0.0665\n",
            "Epoch 2/5 - Loss: 0.0601\n",
            "Epoch 3/5 - Loss: 0.0503\n",
            "Epoch 4/5 - Loss: 0.0429\n",
            "Epoch 5/5 - Loss: 0.0413\n",
            "Avg Recall@10: 0.5529\n",
            "PCA 64D → Recall@10: 0.5529, Query Time: 4.78s\n",
            "\n",
            "--- PCA = 96 ---\n",
            "Epoch 1/5 - Loss: 0.0457\n",
            "Epoch 2/5 - Loss: 0.0407\n",
            "Epoch 3/5 - Loss: 0.0369\n",
            "Epoch 4/5 - Loss: 0.0295\n",
            "Epoch 5/5 - Loss: 0.0270\n",
            "Avg Recall@10: 0.5866\n",
            "PCA 96D → Recall@10: 0.5866, Query Time: 4.90s\n",
            "\n",
            "--- PCA = 128 ---\n",
            "Epoch 1/5 - Loss: 0.0341\n",
            "Epoch 2/5 - Loss: 0.0309\n",
            "Epoch 3/5 - Loss: 0.0293\n",
            "Epoch 4/5 - Loss: 0.0261\n",
            "Epoch 5/5 - Loss: 0.0225\n",
            "Avg Recall@10: 0.5666\n",
            "PCA 128D → Recall@10: 0.5666, Query Time: 5.11s\n",
            "\n",
            "--- PCA = None ---\n",
            "Epoch 1/5 - Loss: 0.0348\n",
            "Epoch 2/5 - Loss: 0.0308\n",
            "Epoch 3/5 - Loss: 0.0292\n",
            "Epoch 4/5 - Loss: 0.0263\n",
            "Epoch 5/5 - Loss: 0.0210\n",
            "Avg Recall@10: 0.5599\n",
            "PCA NoneD → Recall@10: 0.5599, Query Time: 5.16s\n"
          ]
        }
      ]
    }
  ]
}